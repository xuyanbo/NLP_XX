{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用head读取文件的前n行数据，在jupyter notebook上使用!来执行linux命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme,nl\n",
      "1 millón de afectados ante las inundaciones en sri lanka unicef está distribuyendo ayuda de emergencia srilanka,es\n",
      "1 millón de fans en facebook antes del 14 de febrero y paty miki dani y berta se tiran en paracaídas qué harías tú porunmillondefans,es\n",
      "1 satellite galileo sottoposto ai test presso lesaestec nl galileo navigation space in inglese,it\n",
      "10 der welt sind bei,de\n"
     ]
    }
   ],
   "source": [
    "!head -5 data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用open函数读取data.csv文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_f = open('data.csv')\n",
    "lines = in_f.readlines()\n",
    "in_f.close()\n",
    "\n",
    "dataset = [(line.strip()[:-3], line.strip()[-2:]) for line in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示列表中的前五条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1 december wereld aids dag voorlichting in zuidafrika over bieten taboes en optimisme',\n",
       "  'nl'),\n",
       " ('1 millón de afectados ante las inundaciones en sri lanka unicef está distribuyendo ayuda de emergencia srilanka',\n",
       "  'es'),\n",
       " ('1 millón de fans en facebook antes del 14 de febrero y paty miki dani y berta se tiran en paracaídas qué harías tú porunmillondefans',\n",
       "  'es'),\n",
       " ('1 satellite galileo sottoposto ai test presso lesaestec nl galileo navigation space in inglese',\n",
       "  'it'),\n",
       " ('10 der welt sind bei', 'de')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用sklearn数据集划分函数，把原数据集划分为训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = zip(*dataset)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zip()是将对象元素压缩为一个个tuple，zip(*)相当解压。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'c'), ('b', 'd')]\n",
      "[('a', 'b'), ('c', 'd')]\n"
     ]
    }
   ],
   "source": [
    "x = ['a', 'b']\n",
    "y = ['c', 'd']\n",
    "\n",
    "z = list(zip(x, y))\n",
    "print(list(zip(x, y)))\n",
    "print(list(zip(*z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6799"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用正则表达式去除噪声数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trump images are now more popular than cat gits.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_noise(doc):\n",
    "    noise_pattern = re.compile(\"|\".join([\"http\\S+\", \"\\@\\w+\", \"\\#\\w+\"]))\n",
    "    clean_text = re.sub(noise_pattern, \"\", doc)\n",
    "    \n",
    "    return clean_text.strip()\n",
    "\n",
    "remove_noise(\"Trump images are now more popular than cat gits.@Trump #tends http://ww.trumptrends.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn CountVectorizer和TfidfVectorizer**<br/>\n",
    "- CountVectorizer统计每一个训练文本中，每个单词出现的频率；然后构成一个特征矩阵，每一行表示一个训练文本的词频统计结果。其思想是，先根据所有训练文本，不考虑其出现顺序，只将训练文本中每个出现过的词汇单独视为一列特征，构成一个词汇表。<br/>\n",
    "- TfidfVectorizer除了考量某一词汇在当前训练文本中出现的频率之外，同时关注包含这个词汇的其它训练文本数目的倒数。<br/>\n",
    "相比之下，训练文本的数量越多，TfidfVectorizer这种特征量化方式就更有优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(lowercase = True, \n",
    "                       analyzer = 'char_wb',\n",
    "                       ngram_range = (1,3),\n",
    "                       max_features = 1000,\n",
    "                       preprocessor = remove_noise)\n",
    "vec.fit(X_train)\n",
    "\n",
    "def get_features(X):\n",
    "    vec.tranform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用朴素贝叶斯分类器进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "classifier = MultinomialNB(alpha=0.01)\n",
    "classifier.fit(vec.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9907366563740626"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(vec.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 规范化写一个class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump, load\n",
    "\n",
    "class LanguageDetector(object):\n",
    "    # 私有函数\n",
    "    def __init__(self, classifier=MultinomialNB()):\n",
    "        self.classifier = classifier\n",
    "        self.vectorizer = CountVectorizer(max_features=1100, \n",
    "                                          ngram_range=(1,3), \n",
    "                                          preprocessor=self._remove_noise)\n",
    "    \n",
    "    # 数据清洗\n",
    "    def _remove_noise(slef, doc):\n",
    "        noise_pattern = re.compile(\"|\".join([\"http\\S+\", \"\\#\\w+\", \"\\@\\w+\"]))\n",
    "        clean_text = re.sub(noise_pattern, \"\", doc)\n",
    "        \n",
    "        return clean_text.strip()\n",
    "    \n",
    "    # 构建特征\n",
    "    def features(self, X):\n",
    "        return self.vectorizer.transform(X)\n",
    "    \n",
    "    # 拟合数据\n",
    "    def fit(self, X, y):\n",
    "        self.vectorizer.fit(X)\n",
    "        self.classifier.fit(self.features(X), y)\n",
    "    \n",
    "    # 预测类别\n",
    "    def prediction(self, X):\n",
    "        return self.classifier.predict(self.features([X]))\n",
    "    \n",
    "    # 测试集得分\n",
    "    def score(self, X, y):\n",
    "        return self.classifier.score(self.features(X), y)\n",
    "    \n",
    "    # 保存模型\n",
    "    def save_model(self, path):\n",
    "        dump((self.classifier, self.vectorizer), path)\n",
    "    \n",
    "    # 加载模型\n",
    "    def load_model(self, path):\n",
    "        self.classifier, self.vectorizer = load(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练与存储"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9775033083370093\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "in_f = open('data.csv')\n",
    "lines = in_f.readlines()\n",
    "in_f.close()\n",
    "\n",
    "dataset = [(line.strip()[:-3], line.strip()[-2:]) for line in lines]\n",
    "# 划分数据集\n",
    "X, y = zip(*dataset)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# 训练模型\n",
    "language_detector = LanguageDetector()\n",
    "language_detector.fit(X_train, y_train)\n",
    "print(language_detector.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "language_detector.save_model('./model/language_detector.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775033083370093"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载模型\n",
    "load_language_detector = LanguageDetector()\n",
    "load_language_detector.load_model('./model/language_detector.model')\n",
    "load_language_detector.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
